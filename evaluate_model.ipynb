{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94cd9f52",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "We will be evaluating two models. A base Llama 3.2 Instruct model (on zero-shot and ICL) and a fine-tuned Llama 3.2 Instruct model (zero-shot). All models are run locally on my machine via KoboldCPP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e11b54e",
   "metadata": {},
   "source": [
    "# Precision\n",
    "We will evaluate both models for precision. Let's first test the base model with zero shot context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc6c44f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "\n",
    "precision = evaluate.load(\"precision\")\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "dataset = load_dataset(\"json\", data_files=\"test.jsonl\")\n",
    "\n",
    "inputs = [_input for _input in dataset[\"train\"][\"text\"][:50]] # The inputs going to the LLM. Just need a few.\n",
    "references = [label for label in dataset[\"train\"][\"label\"][:50]] # This is the real expected value from the test dataset.\n",
    "\n",
    "# Instruct special tokens specifically for Llama\n",
    "system_tag = \"<|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "user_tag = \"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "assistant_tag = \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "instruction_prompt = system_tag + \"Determine if the user's sentiment is 0 (negative), 1 (neutral), or 2 (positive). \" \\\n",
    "\"Your response should only be a number and nothing else.\\n\"\n",
    "\n",
    "# koboldcpp endpoint\n",
    "KOBOLD_ENDPOINT = \"http://127.0.0.1:5001/api/v1/generate\" # this is specifically the base model's raw text completion. NOT CHAT COMPLETIONS!\n",
    "FT_KOBOLD_ENDPOINT = \"http://127.0.0.1:5002/api/v1/generate\" # the fine-tuned model's endpoint\n",
    "\n",
    "# Sampler settings\n",
    "max_length = 3\n",
    "temperature = 0.1\n",
    "stop_sequence = [\"\\n\", \"<|eot_id|>\", \"<\"]\n",
    "memory = instruction_prompt\n",
    "grammar = \"root ::= \\\"0\\\" | \\\"1\\\" | \\\"2\\\"\" # gbnf grammar to force the model to only output 0, 1, or 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b0d055",
   "metadata": {},
   "source": [
    "### Base Model (Zero Shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8a466c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.2818627450980392}\n",
      "{'accuracy': 0.4}\n",
      "{'f1': 0.3235294117647059}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for _input in inputs:\n",
    "    prompt = user_tag + _input + assistant_tag\n",
    "    body = {\n",
    "        \"memory\": memory,\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_length\": max_length,\n",
    "        \"stop_sequence\": stop_sequence,\n",
    "        \"grammar\": grammar\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(KOBOLD_ENDPOINT, json=body)\n",
    "    except Exception as e:\n",
    "        raise BaseException(\"Could not get response from kobold. Are you sure it's on and running?\")\n",
    "    \n",
    "    predictions.append(response.json()[\"results\"][0][\"text\"])\n",
    "\n",
    "prec_result = precision.compute(references=references, predictions=predictions, average=\"macro\")\n",
    "acc_result = accuracy.compute(references=references, predictions=predictions)\n",
    "f1_result = f1.compute(references=references, predictions=predictions, average=\"macro\")\n",
    "print(prec_result)\n",
    "print(acc_result)\n",
    "print(f1_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff18206",
   "metadata": {},
   "source": [
    "### Base Model (Few Shot)\n",
    "We will be testing one-shot and three-shot example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e0db18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1 results:\n",
      "{'precision': 0.6203044267560397}\n",
      "{'accuracy': 0.5306122448979592}\n",
      "{'f1': 0.5198008898286419}\n",
      "k=3 results:\n",
      "{'precision': 0.3790322580645162}\n",
      "{'accuracy': 0.3829787234042553}\n",
      "{'f1': 0.3405026210299105}\n"
     ]
    }
   ],
   "source": [
    "def eval_k_shot(k: int):\n",
    "    predictions = []\n",
    "    examples = \"Examples:\\n\"\n",
    "\n",
    "    for example, output in zip(inputs[:k], references[:k]):\n",
    "        examples += f\"\\\"{example}\\\" => {output}\\n\\n\"\n",
    "\n",
    "    many_shot_system_prompt = instruction_prompt + \"\\n\\n\" + examples\n",
    "\n",
    "    for _input in inputs[k:]:\n",
    "        prompt = user_tag + _input + assistant_tag\n",
    "        body = {\n",
    "            \"memory\": many_shot_system_prompt,\n",
    "            \"prompt\": prompt,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_length\": max_length,\n",
    "            \"stop_sequence\": stop_sequence,\n",
    "            \"grammar\": grammar\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(KOBOLD_ENDPOINT, json=body)\n",
    "        except Exception as e:\n",
    "            raise BaseException(\"Could not get response from kobold. Are you sure it's on and running?\")\n",
    "        \n",
    "        predictions.append(response.json()[\"results\"][0][\"text\"])\n",
    "\n",
    "    prec_result = precision.compute(references=references[k:], predictions=predictions, average=\"macro\")\n",
    "    acc_result = accuracy.compute(references=references[k:], predictions=predictions)\n",
    "    f1_result = f1.compute(references=references[k:], predictions=predictions, average=\"macro\")\n",
    "    print(f\"k={k} results:\")\n",
    "    print(prec_result)\n",
    "    print(acc_result)\n",
    "    print(f1_result)\n",
    "\n",
    "eval_k_shot(1)\n",
    "eval_k_shot(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f44748",
   "metadata": {},
   "source": [
    "### Fine-tuned (Zero-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f8e066f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.7024864024864025}\n",
      "{'accuracy': 0.7}\n",
      "{'f1': 0.6929693961952026}\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for _input in inputs:\n",
    "    prompt = user_tag + _input + assistant_tag\n",
    "    body = {\n",
    "        \"memory\": memory,\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_length\": max_length,\n",
    "        \"stop_sequence\": stop_sequence,\n",
    "        \"grammar\": grammar\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(FT_KOBOLD_ENDPOINT, json=body)\n",
    "    except Exception as e:\n",
    "        raise BaseException(\"Could not get response from kobold. Are you sure it's on and running?\")\n",
    "    \n",
    "    predictions.append(response.json()[\"results\"][0][\"text\"])\n",
    "\n",
    "prec_result = precision.compute(references=references, predictions=predictions, average=\"macro\")\n",
    "acc_result = accuracy.compute(references=references, predictions=predictions)\n",
    "f1_result = f1.compute(references=references, predictions=predictions, average=\"macro\")\n",
    "print(prec_result)\n",
    "print(acc_result)\n",
    "print(f1_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-class-training-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
